# -*- coding: utf-8 -*-
"""Deep Learning and Attention Mechanism

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UYK3D9VEsYMycmBkwQMOTSYf11NIzfpU
"""

# Cell 1: Install & imports
!pip install -q optuna joblib scikit-learn matplotlib pandas

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import random
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
import joblib

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", DEVICE)

# Cell 2: Load dataset
DATA_PATH = "/content/energydata_complete.csv"  # Upload your CSV first

df = pd.read_csv(DATA_PATH)
print("Loaded shape:", df.shape)
df.head()

# Cell 3: Preprocessing & sequence creation

TARGET = "Appliances"
features = [c for c in df.columns if c not in ["Appliances", "date"]]

# Parse date column
df["date"] = pd.to_datetime(df["date"])
df = df.set_index("date")

# Fill missing
df[features + [TARGET]] = df[features + [TARGET]].ffill().bfill()

# Scale
scaler = StandardScaler()
vals = scaler.fit_transform(df[features + [TARGET]].values)
target_index = len(features)

def create_sequences(data, seq_len, pred_len, target_index):
    Xs, ys = [], []
    for i in range(len(data) - seq_len - pred_len):
        Xs.append(data[i:i+seq_len])
        ys.append(data[i+seq_len:i+seq_len+pred_len, target_index:target_index+1])
    return np.array(Xs), np.array(ys)

SEQ_LEN = 48
PRED_LEN = 12
X, y = create_sequences(vals, SEQ_LEN, PRED_LEN, target_index)

print("X shape:", X.shape, "y shape:", y.shape)

# Split
split = int(len(X) * 0.8)
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

X_train.shape, X_test.shape

# Cell 4: Dataset & DataLoader

class TimeSeriesDataset(Dataset):
    def __init__(self, X, y):
        self.X = X.astype(np.float32)
        self.y = y.astype(np.float32)
    def __len__(self):
        return len(self.X)
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

BATCH = 128
train_loader = DataLoader(TimeSeriesDataset(X_train, y_train), batch_size=BATCH, shuffle=True)
val_loader = DataLoader(TimeSeriesDataset(X_test, y_test), batch_size=BATCH, shuffle=False)

len(train_loader), len(val_loader)

# Cell 5: Seq2Seq + Additive Attention

class AdditiveAttention(nn.Module):
    def __init__(self, enc_dim, dec_dim, attn_dim):
        super().__init__()
        self.W_enc = nn.Linear(enc_dim, attn_dim)
        self.W_dec = nn.Linear(dec_dim, attn_dim)
        self.v = nn.Linear(attn_dim, 1)

    def forward(self, enc_out, dec_hidden):
        score = torch.tanh(self.W_enc(enc_out) + self.W_dec(dec_hidden).unsqueeze(1))
        attn_weights = torch.softmax(self.v(score).squeeze(-1), dim=1)
        context = torch.bmm(attn_weights.unsqueeze(1), enc_out).squeeze(1)
        return context, attn_weights

class Seq2SeqAttention(nn.Module):
    def __init__(self, n_features, hidden=64, pred_len=12):
        super().__init__()
        self.encoder = nn.LSTM(input_size=n_features, hidden_size=hidden, batch_first=True)
        self.attn = AdditiveAttention(hidden, hidden, hidden)
        self.dec_cell = nn.LSTMCell(hidden + 1, hidden)
        self.out = nn.Linear(hidden, 1)
        self.pred_len = pred_len

    def forward(self, x, y_history=None, teacher_forcing=0.5):
        batch = x.size(0)
        enc_out, _ = self.encoder(x)
        dec_h = torch.zeros(batch, enc_out.size(2)).to(x.device)
        dec_c = torch.zeros(batch, enc_out.size(2)).to(x.device)

        last_val = x[:, -1, -1].unsqueeze(-1)
        preds, attn_list = [], []

        for t in range(self.pred_len):
            context, attn_w = self.attn(enc_out, dec_h)
            inp = torch.cat([last_val, context], dim=1)

            dec_h, dec_c = self.dec_cell(inp, (dec_h, dec_c))
            out = self.out(dec_h)
            preds.append(out.unsqueeze(1))
            attn_list.append(attn_w)

            last_val = out if (y_history is None or random.random() > teacher_forcing) else y_history[:, t, :]

        return torch.cat(preds, dim=1), torch.stack(attn_list, dim=1)

# Cell 6: Baseline LSTM

class SimpleLSTM(nn.Module):
    def __init__(self, n_features, hidden=64, pred_len=12):
        super().__init__()
        self.lstm = nn.LSTM(n_features, hidden, batch_first=True)
        self.fc = nn.Linear(hidden, pred_len)

    def forward(self, x, y_history=None, teacher_forcing=0.0):
        out, _ = self.lstm(x)
        out = self.fc(out[:, -1, :]).unsqueeze(-1)
        return out

# Cell 7: Training Function

def train_model(model, train_loader, val_loader, epochs=20, lr=1e-3):
    model.to(DEVICE)
    opt = torch.optim.Adam(model.parameters(), lr=lr)
    loss_fn = nn.MSELoss()

    history = {'train_loss': [], 'val_loss': []}

    for ep in range(1, epochs+1):
        model.train()
        train_loss = 0
        for Xb, yb in train_loader:
            Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)
            opt.zero_grad()
            try:
                preds, _ = model(Xb, y_history=yb)
            except:
                preds = model(Xb)

            loss = loss_fn(preds, yb)
            loss.backward()
            opt.step()
            train_loss += loss.item()
        history['train_loss'].append(train_loss/len(train_loader))

        model.eval()
        val_loss = 0
        with torch.no_grad():
            for Xb, yb in val_loader:
                Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)
                try:
                    preds, _ = model(Xb, y_history=None)
                except:
                    preds = model(Xb)
                val_loss += loss_fn(preds, yb).item()
        history['val_loss'].append(val_loss/len(val_loader))

        print(f"Epoch {ep}: Train {history['train_loss'][-1]:.4f}, Val {history['val_loss'][-1]:.4f}")

    return {"model": model, "history": history}

# Cell 8: Train Seq2Seq Attention Model

n_features = X.shape[2]
attn_model_instance = Seq2SeqAttention(n_features=n_features, hidden=64, pred_len=PRED_LEN)

res_seq = train_model(attn_model_instance, train_loader, val_loader, epochs=15)
attn_model = res_seq["model"]

# Cell 9: Evaluate Attention Model

attn_model.eval()
preds_list, trues_list, attn_list = [], [], []

with torch.no_grad():
    for Xb, yb in val_loader:
        Xb = Xb.to(DEVICE)
        p, att = attn_model(Xb, y_history=None)
        preds_list.append(p.cpu().numpy())
        trues_list.append(yb.numpy())
        attn_list.append(att.cpu().numpy())

preds = np.concatenate(preds_list)
trues = np.concatenate(trues_list)
attns = np.concatenate(attn_list)

# Inverse transform target
t_mean = scaler.mean_[-1]
t_scale = scaler.scale_[-1]

preds_un = preds * t_scale + t_mean
trues_un = trues * t_scale + t_mean

# Plot sample
i = 5
plt.figure(figsize=(8,4))
plt.plot(trues_un[i,:,0], label='True')
plt.plot(preds_un[i,:,0], label='Predicted')
plt.legend()
plt.title("Sample Forecast")
plt.show()

# Attention heatmap
plt.figure(figsize=(8,4))
plt.imshow(attns[i], aspect='auto')
plt.colorbar()
plt.title("Attention Heatmap")
plt.xlabel("Input Time Steps")
plt.ylabel("Prediction Step")
plt.show()

# Cell 10: Train Baseline LSTM

baseline = SimpleLSTM(n_features=n_features, hidden=64, pred_len=PRED_LEN)
baseline = train_model(baseline, train_loader, val_loader, epochs=12)

# Inference baseline
b_preds = []
with torch.no_grad():
    for Xb, yb in val_loader:
        Xb = Xb.to(DEVICE)
        out = baseline(Xb)
        b_preds.append(out.cpu().numpy())

b_preds = np.concatenate(b_preds)
b_preds_un = b_preds * t_scale + t_mean

# Compare horizon 0
pd.DataFrame({
    "true": trues_un[:10,0,0],
    "seq2seq": preds_un[:10,0,0],
    "baseline": b_preds_un[:10,0,0]
})

# Cell 11: Save model

torch.save(attn_model.state_dict(), "/content/seq2seq_attention_model.pth")
joblib.dump(scaler, "/content/scaler.save")

print("Saved model & scaler.")

# Cell 12: Create output folder
OUTPUT_DIR = "/content/project_outputs"
os.makedirs(OUTPUT_DIR, exist_ok=True)

print("Output folder created:", OUTPUT_DIR)

# Cell 13: Save metrics
import json

metrics_data = {
    "seq2seq_attention": metrics_seq,
    "baseline_lstm": metrics_base
}

with open(f"{OUTPUT_DIR}/metrics.json", "w") as f:
    json.dump(metrics_data, f, indent=4)

print("Saved: metrics.json")

from sklearn.metrics import mean_absolute_error, mean_squared_error

def calculate_metrics(trues, preds):
    mae = mean_absolute_error(trues.reshape(-1, 1), preds.reshape(-1, 1))
    rmse = np.sqrt(mean_squared_error(trues.reshape(-1, 1), preds.reshape(-1, 1)))
    # Avoid division by zero for MAPE
    mape = np.mean(np.abs((trues - preds) / trues)) * 100
    return {"MAE": mae, "RMSE": rmse, "MAPE": mape}

# Calculate metrics for Seq2Seq Attention Model
metrics_seq = calculate_metrics(trues_un, preds_un)
print("Seq2Seq Attention Model Metrics:")
for metric, value in metrics_seq.items():
    print(f"  {metric}: {value:.4f}")

# Calculate metrics for Baseline LSTM Model
metrics_base = calculate_metrics(trues_un, b_preds_un)
print("\nBaseline LSTM Model Metrics:")
for metric, value in metrics_base.items():
    print(f"  {metric}: {value:.4f}")

# Cell 14: Save sample prediction plot
sample = 5   # choose any test example

plt.figure(figsize=(8,4))
plt.plot(trues_un[sample,:,0], label='True')
plt.plot(preds_un[sample,:,0], label='Predicted')
plt.title("Seq2Seq Attention â€“ Sample Forecast")
plt.legend()

plt.savefig(f"{OUTPUT_DIR}/prediction_sample.png")
plt.close()

print("Saved: prediction_sample.png")

# Cell 15: Save attention heatmap
plt.figure(figsize=(8,4))
plt.imshow(attns[sample], aspect='auto')
plt.colorbar()
plt.title("Attention Heatmap")

plt.savefig(f"{OUTPUT_DIR}/attention_heatmap.png")
plt.close()

print("Saved: attention_heatmap.png")

# Cell 16: Save loss curve
plt.figure(figsize=(7,4))
plt.plot(res_seq["history"]["train_loss"], label='Train Loss')
plt.plot(res_seq["history"]["val_loss"], label='Val Loss')
plt.legend()
plt.title("Seq2Seq Attention - Loss Curve")

plt.savefig(f"{OUTPUT_DIR}/loss_curve.png")
plt.close()

print("Saved: loss_curve.png")

# Cell 17: Save predictions and true values
pd.DataFrame(preds_un[:,:,0]).to_csv(f"{OUTPUT_DIR}/predictions.csv", index=False)
pd.DataFrame(trues_un[:,:,0]).to_csv(f"{OUTPUT_DIR}/true_values.csv", index=False)

print("Saved: predictions.csv, true_values.csv")

# Cell 18: Save model & scaler inside output folder
torch.save(attn_model.state_dict(), f"{OUTPUT_DIR}/seq2seq_attention_model.pth")
joblib.dump(scaler, f"{OUTPUT_DIR}/scaler.save")

print("Saved: model + scaler in project_outputs/")